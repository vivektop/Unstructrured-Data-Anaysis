{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis - Unsupervised Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  A very, very, very slow-moving, aimless movie ...          0\n",
       "1  Not sure who was more lost - the flat characte...          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb = pd.read_csv('E:/Term3/Unstructured/imdb_sentiment.csv')\n",
    "imdb.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER\n",
    "Valence Aware Dictionary and sEentiment Reasoner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    -sentence polarity = No.of.Positive words - No.of.Negative words ( In that sentence) \n",
    "    -->In Traditional methods , +ve is 1 and -ve is 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.6369, 'neg': 0.0, 'neu': 0.192, 'pos': 0.808}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = SentimentIntensityAnalyzer()\n",
    "sentiment.polarity_scores('I Love India')\n",
    "# Returns % of positive score,% of negative score and % of neutral\n",
    "# compound >0 +ve score , < 0 -ve ,~0 neutral ( between -1 to 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.128, 'neg': 0.374, 'neu': 0.202, 'pos': 0.424}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.polarity_scores('I Love India I hate apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    sentiment = SentimentIntensityAnalyzer()\n",
    "    compound = sentiment.polarity_scores(text)['compound']\n",
    "    if compound > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "imdb['sentiment_vader'] = imdb['review'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7941176470588235"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(imdb['sentiment'],imdb['sentiment_vader'])\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon = pd.read_csv('E:/Term3/Unstructured/amazon_reviews.csv')\n",
    "amazon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>We got this GPS for my husband who is an (OTR)...</td>\n",
       "      <td>06 2, 2013</td>\n",
       "      <td>AO94DHGC771SJ</td>\n",
       "      <td>amazdnu</td>\n",
       "      <td>Gotta have GPS!</td>\n",
       "      <td>1.370131e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>[12, 15]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I'm a professional OTR truck driver, and I bou...</td>\n",
       "      <td>11 25, 2010</td>\n",
       "      <td>AMO214LNFCEI4</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>Very Disappointed</td>\n",
       "      <td>1.290643e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>[43, 45]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Well, what can I say.  I've had this unit in m...</td>\n",
       "      <td>09 9, 2010</td>\n",
       "      <td>A3N7T0DY83Y4IG</td>\n",
       "      <td>C. A. Freeman</td>\n",
       "      <td>1st impression</td>\n",
       "      <td>1.283990e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>[9, 10]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Not going to write a long review, even thought...</td>\n",
       "      <td>11 24, 2010</td>\n",
       "      <td>A1H8PY3QHMQQA0</td>\n",
       "      <td>Dave M. Shaw \"mack dave\"</td>\n",
       "      <td>Great grafics, POOR GPS</td>\n",
       "      <td>1.290557e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0528881469</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I've had mine for a year and here's what we go...</td>\n",
       "      <td>09 29, 2011</td>\n",
       "      <td>A24EV6RXELQZ63</td>\n",
       "      <td>Wayne Smith</td>\n",
       "      <td>Major issues, only excuses for support</td>\n",
       "      <td>1.317254e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        asin   helpful  overall  \\\n",
       "0           0  0528881469    [0, 0]      5.0   \n",
       "1           1  0528881469  [12, 15]      1.0   \n",
       "2           2  0528881469  [43, 45]      3.0   \n",
       "3           3  0528881469   [9, 10]      2.0   \n",
       "4           4  0528881469    [0, 0]      1.0   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  We got this GPS for my husband who is an (OTR)...   06 2, 2013   \n",
       "1  I'm a professional OTR truck driver, and I bou...  11 25, 2010   \n",
       "2  Well, what can I say.  I've had this unit in m...   09 9, 2010   \n",
       "3  Not going to write a long review, even thought...  11 24, 2010   \n",
       "4  I've had mine for a year and here's what we go...  09 29, 2011   \n",
       "\n",
       "       reviewerID              reviewerName  \\\n",
       "0   AO94DHGC771SJ                   amazdnu   \n",
       "1   AMO214LNFCEI4           Amazon Customer   \n",
       "2  A3N7T0DY83Y4IG             C. A. Freeman   \n",
       "3  A1H8PY3QHMQQA0  Dave M. Shaw \"mack dave\"   \n",
       "4  A24EV6RXELQZ63               Wayne Smith   \n",
       "\n",
       "                                  summary  unixReviewTime  \n",
       "0                         Gotta have GPS!    1.370131e+09  \n",
       "1                       Very Disappointed    1.290643e+09  \n",
       "2                          1st impression    1.283990e+09  \n",
       "3                 Great grafics, POOR GPS    1.290557e+09  \n",
       "4  Major issues, only excuses for support    1.317254e+09  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    got gp husband otr road trucker  impress ship ...\n",
       "1    im profession otr truck driver bought tnd  tru...\n",
       "2    well say  ive unit truck four day  prior garmi...\n",
       "3    go write long review even thought unit deserv ...\n",
       "4    ive mine year here got tri rout non truck rout...\n",
       "Name: reviewText, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = amazon['reviewText'].fillna('').str.lower().str.replace('[^a-z ]','')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def clean_sentence(text):\n",
    "    words = text.split(' ')\n",
    "    words_clean = [stemmer.stem(word) for word in words if word not in stopwords]\n",
    "    return ' '.join(words_clean)\n",
    "docs_clean = docs.apply(clean_sentence)\n",
    "docs_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 7172)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(docs_clean)\n",
    "df_dtm = pd.DataFrame(vectorizer.transform(docs_clean).toarray(),columns=vectorizer.get_feature_names())\n",
    "df_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.35781834],\n",
       "       [0.35781834, 1.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cosine similarity between 2 words(vectors)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity([df_dtm['tablet'],df_dtm['ipad']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aac</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>abnorm</th>\n",
       "      <th>aboutdo</th>\n",
       "      <th>aboutif</th>\n",
       "      <th>absenc</th>\n",
       "      <th>absent</th>\n",
       "      <th>...</th>\n",
       "      <th>zbox</th>\n",
       "      <th>zero</th>\n",
       "      <th>zillion</th>\n",
       "      <th>zip</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zippi</th>\n",
       "      <th>zippier</th>\n",
       "      <th>zirco</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zune</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaa</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aac</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100504</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abil</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.20324</td>\n",
       "      <td>0.100504</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.201008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.252714</td>\n",
       "      <td>0.071067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abl</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.203240</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.065233</td>\n",
       "      <td>0.195698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.065233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106525</td>\n",
       "      <td>0.065233</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130466</td>\n",
       "      <td>0.260931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257755</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7172 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aaa       aac  abandon      abil      abl    abnorm   aboutdo  \\\n",
       "aaa      1.0  0.000000      0.0  0.000000  0.00000  0.000000  0.000000   \n",
       "aac      0.0  1.000000      0.0  0.100504  0.00000  0.000000  0.000000   \n",
       "abandon  0.0  0.000000      1.0  0.000000  0.00000  0.000000  0.000000   \n",
       "abil     0.0  0.100504      0.0  1.000000  0.20324  0.100504  0.000000   \n",
       "abl      0.0  0.000000      0.0  0.203240  1.00000  0.065233  0.195698   \n",
       "\n",
       "         aboutif  absenc    absent    ...     zbox      zero   zillion  \\\n",
       "aaa          0.0     0.0  0.000000    ...      0.0  0.000000  0.000000   \n",
       "aac          0.0     0.0  0.000000    ...      0.0  0.000000  0.000000   \n",
       "abandon      0.0     0.0  0.000000    ...      0.0  0.000000  0.000000   \n",
       "abil         0.0     0.0  0.201008    ...      0.0  0.000000  0.000000   \n",
       "abl          0.0     0.0  0.065233    ...      0.0  0.106525  0.065233   \n",
       "\n",
       "              zip  zipper     zippi   zippier  zirco      zoom      zune  \n",
       "aaa      0.000000     0.0  0.000000  0.000000    0.0  0.000000  0.000000  \n",
       "aac      0.000000     0.0  0.000000  0.000000    0.0  0.000000  0.000000  \n",
       "abandon  0.000000     0.0  0.000000  0.000000    0.0  0.000000  0.000000  \n",
       "abil     0.116052     0.0  0.000000  0.201008    0.0  0.252714  0.071067  \n",
       "abl      0.037662     0.0  0.130466  0.260931    0.0  0.257755  0.000000  \n",
       "\n",
       "[5 rows x 7172 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_mat = cosine_similarity(df_dtm.T) # If not transposed , will give document similarity\n",
    "sim_mat = pd.DataFrame(sim_mat,columns=df_dtm.columns,index=df_dtm.columns)\n",
    "sim_mat.head(5)\n",
    "#Symantec analysis\n",
    "#sim_mat['tablet'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing a function to get the similar words of  a  given word.\n",
    "\n",
    "def get_similar_words(input_word,sim_mat):  \n",
    "    cos_vals = sim_mat[input_word].sort_values(ascending = False) # we can drop the similar words\n",
    "    similar_words = cos_vals.drop(input_word).head(5)\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tablet        0.625789\n",
       "android       0.555623\n",
       "marketplac    0.536400\n",
       "bird          0.532974\n",
       "doesnt        0.508315\n",
       "Name: app, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar_words('app',sim_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Topic Modelling and Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOPIC MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tag the data , Multiple tagging.Similar to clustering but has more advantages.\n",
    "- ### Popular Methods\n",
    "- Latent Dirichlet Allocation (LDA) widely used\n",
    "- Latent Sematic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading https://files.pythonhosted.org/packages/54/ee/c1f685caa83ee9b8f54573b51648af61b01377bcc5981a18704f5247cce7/gensim-3.7.1-cp36-cp36m-win_amd64.whl (24.1MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.0.0)\n",
      "Collecting smart-open>=1.7.0 (from gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/ff/c8/de7dcf34d4b5f2ae94fe1055e0d6418fb97a63c9dc3428edd264704983a2/smart_open-1.8.0.tar.gz (40kB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim) (2.48.0)\n",
      "Collecting bz2file (from smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim) (2.18.4)\n",
      "Collecting boto3 (from smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/9f/f7/325ba3c39b2d0c8f361a3ba3de913a6b57bbb97cc520a9406cd1a8cdf9fd/boto3-1.9.95-py2.py3-none-any.whl (128kB)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (2018.1.18)\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.3.0,>=0.2.0 (from boto3->smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/de/5737f602e22073ecbded7a0c590707085e154e32b68d86545dcc31004c02/s3transfer-0.2.0-py2.py3-none-any.whl (69kB)\n",
      "Collecting botocore<1.13.0,>=1.12.95 (from boto3->smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/e5/cb3e70e65750c5dd37a1130537139d594d7133931fdb1b5933f1b8ebcac1/botocore-1.12.95-py2.py3-none-any.whl (5.3MB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.95->boto3->smart-open>=1.7.0->gensim) (2.6.1)\n",
      "Requirement already satisfied: docutils>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.95->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "Building wheels for collected packages: smart-open, bz2file\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\venkat\\AppData\\Local\\pip\\Cache\\wheels\\f7\\a6\\ff\\9ab5842c14e50e95a06a4675b0b4a689c9cab6064dac2b01d0\n",
      "  Building wheel for bz2file (setup.py): started\n",
      "  Building wheel for bz2file (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\venkat\\AppData\\Local\\pip\\Cache\\wheels\\81\\75\\d6\\e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: bz2file, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.9.95 botocore-1.12.95 bz2file-0.98 gensim-3.7.1 jmespath-0.9.3 s3transfer-0.2.0 smart-open-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gensim doesnt store all the 0s in DTM (memory ineffecient), instead it stores the non zero values as a list .\n",
    "- (inside ,a list for each document)\n",
    "- bag of words at document level\n",
    "- List for a corpus , inside the list number of lists is number of rows(documents),every tuple in a list consists of unique \n",
    "  terms and the frequency of each term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon = pd.read_csv('E:/Term3/Unstructured/amazon_reviews_big.csv')\n",
    "amazon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_clean =[]\n",
    "docs = amazon['reviewText'].fillna('').str.lower().str.replace('[^a-z ]','')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(['','use','good','like','great','work','one'])\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "for doc in docs:\n",
    "    words = doc.split(' ')\n",
    "    words_clean = [stemmer.stem(word) for word in words if stemmer.stem(word) not in stopwords]\n",
    "    docs_clean.append(words_clean)\n",
    "len(docs_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x15a1ab7908>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create ID's to each word in docs_clean\n",
    "dictionary = gensim.corpora.Dictionary(docs_clean)\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(zip(dictionary.keys(),dictionary.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_bow =[]\n",
    "for doc in docs_clean:\n",
    "    doc_bow = dictionary.doc2bow(doc) #Takes 1 document at a time and identifies its frequency along with its ID(from dictionary)\n",
    "    docs_bow.append(doc_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 5),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 3),\n",
       " (11, 2),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 1),\n",
       " (16, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 2),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 1),\n",
       " (24, 1),\n",
       " (25, 1),\n",
       " (26, 1),\n",
       " (27, 1),\n",
       " (28, 2),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (32, 1),\n",
       " (33, 1),\n",
       " (34, 2),\n",
       " (35, 2),\n",
       " (36, 1),\n",
       " (37, 1),\n",
       " (38, 1),\n",
       " (39, 1),\n",
       " (40, 1),\n",
       " (41, 1),\n",
       " (42, 1),\n",
       " (43, 1),\n",
       " (44, 1),\n",
       " (45, 1),\n",
       " (46, 1),\n",
       " (47, 1),\n",
       " (48, 1),\n",
       " (49, 1),\n",
       " (50, 1)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.doc2bow(docs_clean[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaModel(docs_bow,id2word = dictionary,num_topics=6) # Id mapped to each word , printing the each\n",
    "# word id2word is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.98624134)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.get_document_topics(docs_bow[0]) #This is relation between Topic and documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"case\" + 0.008*\"fit\" + 0.007*\"sound\" + 0.007*\"well\" + 0.007*\"get\" + 0.007*\"look\" + 0.007*\"use\" + 0.007*\"would\" + 0.006*\"headphon\" + 0.006*\"realli\"'),\n",
       " (1,\n",
       "  '0.015*\"cabl\" + 0.015*\"sound\" + 0.013*\"speaker\" + 0.013*\"tv\" + 0.012*\"work\" + 0.010*\"price\" + 0.009*\"product\" + 0.009*\"qualiti\" + 0.008*\"would\" + 0.008*\"get\"'),\n",
       " (2,\n",
       "  '0.013*\"pictur\" + 0.011*\"video\" + 0.010*\"set\" + 0.009*\"screen\" + 0.008*\"get\" + 0.007*\"featur\" + 0.007*\"qualiti\" + 0.007*\"monitor\" + 0.007*\"record\" + 0.006*\"look\"'),\n",
       " (3,\n",
       "  '0.049*\"camera\" + 0.025*\"len\" + 0.011*\"canon\" + 0.010*\"mm\" + 0.010*\"bag\" + 0.009*\"use\" + 0.008*\"take\" + 0.008*\"shoot\" + 0.007*\"flash\" + 0.007*\"light\"'),\n",
       " (4,\n",
       "  '0.031*\"batteri\" + 0.021*\"charg\" + 0.016*\"work\" + 0.015*\"connect\" + 0.014*\"mous\" + 0.014*\"cabl\" + 0.013*\"router\" + 0.012*\"devic\" + 0.012*\"wireless\" + 0.011*\"use\"'),\n",
       " (5,\n",
       "  '0.015*\"drive\" + 0.010*\"card\" + 0.008*\"use\" + 0.008*\"comput\" + 0.008*\"usb\" + 0.008*\"work\" + 0.006*\"get\" + 0.006*\"devic\" + 0.006*\"instal\" + 0.006*\"need\"')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now relation between Topics and words\n",
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Once the model we build is good enough , we will go for tagging\n",
    "doc2topic_prob =pd.DataFrame(lda_model.get_document_topics(docs_bow[0]),columns = ['topic','prob'])\n",
    "doc2topic_prob.sort_values('prob',ascending = False).iloc[0]['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "for doc_bow in docs_bow:\n",
    "    doc2topic_prob = lda_model.get_document_topics(doc_bow)\n",
    "    doc2topic_prob = pd.DataFrame(doc2topic_prob,\n",
    "                                 columns=['topic','prob'])\n",
    "    topic = doc2topic_prob.sort_values('prob',\n",
    "                                      ascending=False).iloc[0]['topic']\n",
    "    topics.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame constructor not properly called!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-9bb2d3dd098e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc_bow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs_bow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdoc2topic_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs_bow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdoc2topic_prob\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc2topic_prob\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'topic'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'prob'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtopic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc2topic_prob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'prob'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mascending\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'topic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtopics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    402\u001b[0m                                          dtype=values.dtype, copy=False)\n\u001b[0;32m    403\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataFrame constructor not properly called!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmgr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: DataFrame constructor not properly called!"
     ]
    }
   ],
   "source": [
    "# Repeat for all the documents\n",
    "\n",
    "#topics = []\n",
    "#for doc_bow in docs_bow:\n",
    "#    doc2topic_prob = lda_model.get_document_topics(docs_bow)\n",
    "#    doc2topic_prob =pd.DataFrame(doc2topic_prob,columns = ['topic','prob'])\n",
    "#    topic = doc2topic_prob.sort_values('prob',ascending = False).iloc[0]['topic']\n",
    "#    topics.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x158fdae9b0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x151433bac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "amazon['topics'] = topics\n",
    "amazon['topics'].value_counts().plot.bar(color = 'steelblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ----------------------------------------------------------------------- EXERCISE--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    " - Use hotstar data,predict sentiment score for the reviews column using vader package \n",
    " - Convert score to labels:\n",
    "     - Positive(score > 0.05)\n",
    "     - Negative(score <-0.05)\n",
    "     - Neutral(-0.05 <=score<=0.05)\n",
    " - calculate accuracy score \n",
    " - Compare the accuracy with supervised models and comment whether supervised or unsupervised gives best accuracy.\n",
    " - Create a multi line chart for comparing no.of.positive,negative and neutral reviews for each day ( we have one week data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>UserName</th>\n",
       "      <th>Created_Date</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Lower_Case_Reviews</th>\n",
       "      <th>Sentiment_Manual_BP</th>\n",
       "      <th>Sentiment_Manual</th>\n",
       "      <th>Review_Length</th>\n",
       "      <th>DataSource</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sentiment_Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8/10/2017</td>\n",
       "      <td>Hh</td>\n",
       "      <td>hh</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2</td>\n",
       "      <td>Google_PlayStore</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8/11/2017</td>\n",
       "      <td>No</td>\n",
       "      <td>no</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>2</td>\n",
       "      <td>Google_PlayStore</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>asadynwa</td>\n",
       "      <td>8/12/2017</td>\n",
       "      <td>@hotstar_helps during paymnt for premium subsc...</td>\n",
       "      <td>@hotstar_helps during paymnt for premium subsc...</td>\n",
       "      <td>Help</td>\n",
       "      <td>Negative</td>\n",
       "      <td>140</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>jineshroxx</td>\n",
       "      <td>8/11/2017</td>\n",
       "      <td>@hotstartweets I am currently on Jio network a...</td>\n",
       "      <td>@hotstartweets i am currently on jio network a...</td>\n",
       "      <td>Help</td>\n",
       "      <td>Negative</td>\n",
       "      <td>140</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>YaminiSachar</td>\n",
       "      <td>8/5/2017</td>\n",
       "      <td>@hotstartweets the episodes of Sarabhai vs Sar...</td>\n",
       "      <td>@hotstartweets the episodes of sarabhai vs sar...</td>\n",
       "      <td>Help</td>\n",
       "      <td>Negative</td>\n",
       "      <td>140</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID      UserName Created_Date  \\\n",
       "0   1           NaN    8/10/2017   \n",
       "1   2           NaN    8/11/2017   \n",
       "2   3      asadynwa    8/12/2017   \n",
       "3   4    jineshroxx    8/11/2017   \n",
       "4   5  YaminiSachar     8/5/2017   \n",
       "\n",
       "                                             Reviews  \\\n",
       "0                                                 Hh   \n",
       "1                                                 No   \n",
       "2  @hotstar_helps during paymnt for premium subsc...   \n",
       "3  @hotstartweets I am currently on Jio network a...   \n",
       "4  @hotstartweets the episodes of Sarabhai vs Sar...   \n",
       "\n",
       "                                  Lower_Case_Reviews Sentiment_Manual_BP  \\\n",
       "0                                                 hh            Negative   \n",
       "1                                                 no            Negative   \n",
       "2  @hotstar_helps during paymnt for premium subsc...                Help   \n",
       "3  @hotstartweets i am currently on jio network a...                Help   \n",
       "4  @hotstartweets the episodes of sarabhai vs sar...                Help   \n",
       "\n",
       "  Sentiment_Manual  Review_Length        DataSource  Year  Month  Date  \\\n",
       "0         Negative              2  Google_PlayStore  2017      8    10   \n",
       "1         Negative              2  Google_PlayStore  2017      8    11   \n",
       "2         Negative            140           Twitter  2017      8    12   \n",
       "3         Negative            140           Twitter  2017      8    11   \n",
       "4         Negative            140           Twitter  2017      8     5   \n",
       "\n",
       "  Sentiment_Polarity  \n",
       "0            Neutral  \n",
       "1            Neutral  \n",
       "2           Negative  \n",
       "3           Positive  \n",
       "4            Neutral  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs = pd.read_csv('E:/Term3/Unstructured/hotstar.allreviews_Sentiments.csv')\n",
    "hs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    sentiment = SentimentIntensityAnalyzer()\n",
    "    compound = sentiment.polarity_scores(text)['compound']\n",
    "    if compound > 0.05:\n",
    "        return 'Positive'\n",
    "    elif compound < -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "    \n",
    "hs['sentiment_vader'] = hs['Reviews'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5503661191371463"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(hs['Sentiment_Manual'],hs['sentiment_vader'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
